{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devsjee/LLM-payground/blob/main/Chat_with_Github_Repo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfN8aiXeXrYU"
      },
      "source": [
        "We know that the publicly available applications of LLMs like ChatGPT are very alluring in terms of their ability to create meaningul conversations. It is even more interesting to learn that these LLMs can be easily used for creating a chatbot to assist us in exploring or understanding a specific dataset of our choice. For example, let us say I want to investigate some github repository and understand its code flow. Then, I can create a LLM based chatbot to answer my queries pertaining to that github repo.\n",
        "\n",
        "\n",
        "Broadly speaking, this application will have the following three components:\n",
        "\n",
        "1.  A front end to receive inputs from user and show the outputs: A simple front end would be the Python terminal itself. We can use Python input() for getting inputs from the command line and also show outputs on same terminal itself.  We will begin with this simple front end first and subsequently use an advanced library called streamlit for creating a web app like front end.\n",
        "\n",
        "2.  A back end for storing and accessing data: A simple backend for our application would be a Python list or a Python dictionary. But, will it be efficient for storing and querying embedding data? imagine storing the embeddings in a python list and then trying to fetch similar embeddings. For similarity based search, we need to compute cosine distance between the query and all embeddings in our list. This is computationally expensive though there might be techniques to improve the search efficiency.Fortunately, for us, there are options like DeepLake, PineCone, etc that do this embedding storage and retrieval efficiently for us. I will use DeepLake for this experiment as I am basically following DeepLake's tutorial to buidl this custom LLM chatbot.\n",
        "\n",
        "3. A process engine for chatbot : Creating a custom chatbot based on LLM involves three main steps:\n",
        "\n",
        "> Indented block\n",
        "1. Processing the custom database and creating chunks of data i.e. creating a collection of data pieces of uniform length.\n",
        "2. Converting the chunks to embeddings and storing them in a vector database. A vector database is a type of database that facilitates the efficient storage and retrieval of the high dimensional embedding vectors.\n",
        "3. Using a LLM model to answer queries pertaining to this database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSQwjbQMc974",
        "outputId": "5ba54f79-f787-48e9-9595-344589c3e7b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the github url: https://github.com/devsjee/LaTeX-Tutorial\n",
            "Enter the list of file extensions to be included in the chat database separated by spaces. Eg: .py .js .pdf.pdf .tex\n"
          ]
        }
      ],
      "source": [
        "#Front end for chatbot\n",
        "#In this application, there will be two instances where the user will need to interact with our program.\n",
        "# 1.  To provide the github url for processing\n",
        "# 2.  To engage in chatting about the github repo after it is processed by our program\n",
        "\n",
        "\n",
        "#Get the user input for github url a\n",
        "repo_url= input('Enter the github url: ')\n",
        "include_file_extensions= input('Enter the list of file extensions to be included in the chat database separated by spaces. Eg: .py .js .pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(include_file_extensions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPZoFibr2WvJ",
        "outputId": "213db914-c7f7-4ff7-cbb4-f8cd211203eb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".pdf .tex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koNyZ0YaMWpl",
        "outputId": "17aa8754-c57a-4128-a2f1-3fd389879e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key··········\n",
            "Enter Activeloop API token··········\n",
            "Enter Activeloop Username··········\n"
          ]
        }
      ],
      "source": [
        "# Next we will set the environment variables to hold the key values that we will need access to.\n",
        "# for OpenAI API key: Signup for OpenAI at https://openai.com/ and get your api key from your account/profile page\n",
        "# for Activeloop API token: Register at activeloop site https://app.activeloop.ai/register/ and get the token from\n",
        "# for Activeloop User name: your registered username with activeloop\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Enter OpenAI API key\")\n",
        "os.environ['ACTIVELOOP_TOKEN'] = getpass.getpass(\"Enter Activeloop API token\")\n",
        "os.environ['ACTIVELOOP_USERNAME'] = getpass.getpass(\"Enter Activeloop Username\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "uyUvSzMELkhh"
      },
      "outputs": [],
      "source": [
        "def extract_repo_name(repo_url):\n",
        "    \"\"\"Extract the repository name from the given repository URL.\"\"\"\n",
        "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
        "    return repo_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glrCB4_tLlhJ",
        "outputId": "5146e21b-767b-4632-f100-a45c64bfe855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repo name:  LaTeX-Tutorial\n",
            "al uname:  devsjee\n",
            "al dataset path: hub://devsjee/LaTeX-Tutorial\n"
          ]
        }
      ],
      "source": [
        "repo_name= extract_repo_name(repo_url)\n",
        "activeloop_username = os.environ.get(\"ACTIVELOOP_USERNAME\")\n",
        "activeloop_dataset_path = f\"hub://{activeloop_username}/{repo_name}\"\n",
        "activeloop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
        "\n",
        "print(\"repo name: \", repo_name)\n",
        "print(\"al uname: \",activeloop_username)\n",
        "print(\"al dataset path:\" ,activeloop_dataset_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important lesson that I learned from trying implement this application from scratch is that all the libraries used here are frequently updated and hence we need to ensure that we always have the latest versions installed on our system. so the '--upgrade' option used below was very essential to my successful creation of this chatbot."
      ],
      "metadata": {
        "id": "WAwSKq5YpF6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "UtRvEvC2XknY"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade DeepLake\n",
        "!pip install -q --upgrade langchain\n",
        "!pip install -q --upgrade langchain_community\n",
        "!pip install -q --upgrade langchain-openai\n",
        "!pip install -q --upgrade pathspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BEtqD68Wq6us"
      },
      "outputs": [],
      "source": [
        "#While running the code from peterw repo, there was an error related to base64 encoding. It is mainly about how the activeloop token is encoded.\n",
        "# the below will do the necessary encoding changes to activeloop token\n",
        "import base64\n",
        "\n",
        "# Remove any trailing padding characters\n",
        "activeloop_token = activeloop_token.rstrip(\"=\")\n",
        "\n",
        "# Add the missing padding characters\n",
        "padding_length = 4 - len(activeloop_token) % 4\n",
        "activeloop_token += \"=\" * padding_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-FnCRpJgxUkP"
      },
      "outputs": [],
      "source": [
        "#When I had to repeat this experiment again and again for debugging, I had to delete the local repository folder entirely but Colab does not allow to delete a non-empty directory\n",
        "# So used the below code for recursive deletion of the local repo folder.\n",
        "import os\n",
        "for root, dirs, files in os.walk(\"devsjee/\", topdown=False):\n",
        "    for name in files:\n",
        "        os.remove(os.path.join(root, name))\n",
        "    for name in dirs:\n",
        "        os.rmdir(os.path.join(root, name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX56v908RQwY",
        "outputId": "3666cd2c-2f7c-44ef-f1b1-33632f267082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n",
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/devsjee/LaTeX-Tutorial\n",
            "hub://devsjee/LaTeX-Tutorial loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tensor(key='text')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#a dataset will be created in the online data hub at activeloop site afer this step.\n",
        "#Login to your activeloop account and see the dataset created or using the link that\n",
        "#appears in the output upon succesful running of this cell.\n",
        "import deeplake\n",
        "ds = deeplake.empty(\n",
        "        activeloop_dataset_path,\n",
        "        token=activeloop_token,\n",
        "        overwrite=True,\n",
        "\n",
        "    )\n",
        "\n",
        "ds.create_tensor(\"ids\")\n",
        "ds.create_tensor(\"metadata\")\n",
        "ds.create_tensor(\"embedding\")\n",
        "ds.create_tensor(\"text\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/repos"
      ],
      "metadata": {
        "id": "jYrKoknN1LB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/repos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGipi4Rozt3j",
        "outputId": "b65fea03-0da7-4b32-a061-db221f92d498"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/repos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/devsjee/LaTeX-Tutorial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEQWHVL50TUq",
        "outputId": "9d6afb12-42de-436d-affb-e1169ada390f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LaTeX-Tutorial'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 17 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (17/17), 481.72 KiB | 7.90 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxG1owlqUxa0",
        "outputId": "a20a0f3a-90ba-4172-e4c3-9acb37d485c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['git', 'clone', 'github.com/devsjee/LaTeX-Tutorial', '/content/repos/LaTeX-Tutorial'], returncode=128)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# we will clone the given github url and store it locally. this did not work for me. So I had used the earlier two cells for creating a local folder and cloning\n",
        "# url directly into it.\n",
        "import subprocess\n",
        "\n",
        "repo_destination = \"/content/repos/LaTeX-Tutorial\"\n",
        "\n",
        "subprocess.run([\"git\",\"clone\",repo_url,repo_destination])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "El-0EInKLK0x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathspec\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import DeepLake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "FE1AM_w0U17I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a24e2f-8cc8-47f2-dfc6-6be5fe655521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chapter1.png\n",
            "knuth.jpg\n",
            "title.png\n",
            "README.md\n",
            "tutorial_slides.tex\n",
            "tutorial_slides.pdf\n",
            "hello.png\n",
            "bib.jpeg\n",
            "chapter3.png\n",
            "chapter2.png\n"
          ]
        }
      ],
      "source": [
        "# we will load only the relevant files fom the clones repo to the 'docs' variable\n",
        "# relevant files are those files whose extensions match any of the given '--to include file extensions '' given by the user\n",
        "docs = []\n",
        "\n",
        "# Load .gitignore rules\n",
        "gitignore_path = os.path.join(repo_destination, \".gitignore\")\n",
        "\n",
        "if os.path.isfile(gitignore_path):\n",
        "    with open(gitignore_path, \"r\") as gitignore_file:\n",
        "        gitignore = gitignore_file.read()\n",
        "    spec = pathspec.PathSpec.from_lines(\n",
        "        pathspec.patterns.GitWildMatchPattern, gitignore.splitlines()\n",
        "    )\n",
        "else:\n",
        "    spec = None\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(repo_destination):\n",
        "    # Remove dot directories from the list of directory names\n",
        "    dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n",
        "\n",
        "    for file in filenames:\n",
        "        print(file)\n",
        "        file_path = os.path.join(dirpath, file)\n",
        "\n",
        "        # Skip dotfiles\n",
        "        if file.startswith(\".\"):\n",
        "            continue\n",
        "\n",
        "        # Skip files that match .gitignore rules\n",
        "        if spec and spec.match_file(file_path):\n",
        "            continue\n",
        "\n",
        "        if include_file_extensions and os.path.splitext(file)[1] not in include_file_extensions:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
        "            docs.extend(loader.load_and_split())\n",
        "        except Exception:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDS8-mRJL8Y-",
        "outputId": "a987e6ff-f80d-4696-9719-fd77b7cf454d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "875Ir0UOU3ci",
        "outputId": "16fa26af-6e83-43ba-e43d-40ca6afbc857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n"
          ]
        }
      ],
      "source": [
        "#we are splitting the collection of documents into collection of text chunks where each chunk is of 1000 Characters.\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "\n",
        "print(len(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U6heJx30azT",
        "outputId": "dea90bde-e115-4e75-df70-5d871ec4edab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-33NUF8pU4s-",
        "outputId": "07c57a84-435f-487d-f879-0c1d164c2a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://devsjee/LaTeX-Tutorial already exists, loading from the storage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 27 embeddings in 1 batches of size 27:: 100%|██████████| 1/1 [00:38<00:00, 38.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://devsjee/LaTeX-Tutorial', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
            "\n",
            "  tensor     htype     shape      dtype  compression\n",
            "  -------   -------   -------    -------  ------- \n",
            " embedding  generic  (54, 1536)  float32   None   \n",
            "    ids      text     (54, 1)      str     None   \n",
            " metadata    json     (54, 1)      str     None   \n",
            "   text      text     (54, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#creating a dataset of vector embeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "db = DeepLake.from_documents(texts,embeddings,dataset_path=activeloop_dataset_path)\n",
        "#db.add_documents(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R81UazlPw_8v",
        "outputId": "1d2afbff-e814-46db-a8b1-03c45ec441c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.deeplake:Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://devsjee/LaTeX-Tutorial already exists, loading from the storage\n"
          ]
        }
      ],
      "source": [
        "# Create a 'read-only' instance of DeepLake with the specified dataset path and embeddings; this is used for search and retrieval purpose at the time of chatting\n",
        "db = DeepLake(\n",
        "        dataset_path=activeloop_dataset_path,\n",
        "        read_only=True,\n",
        "        embedding_function=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "5JQ2Hplkyg9d"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8RFL9dU6xtwU"
      },
      "outputs": [],
      "source": [
        "def search_db(db, query):\n",
        "    \"\"\"Search for a response to the query in the DeepLake database.\"\"\"\n",
        "    # Create a retriever from the DeepLake instance\n",
        "    retriever = db.as_retriever()\n",
        "    # Set the search parameters for the retriever\n",
        "    retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
        "    retriever.search_kwargs[\"fetch_k\"] = 100\n",
        "    #retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
        "    retriever.search_kwargs[\"k\"] = 10\n",
        "    # Create a ChatOpenAI model instance\n",
        "    model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "    # Create a RetrievalQA instance from the model and retriever\n",
        "    qa = RetrievalQA.from_llm(model, retriever=retriever)\n",
        "    # Return the result of the query\n",
        "    return qa.run(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH5r42uAxV4Y",
        "outputId": "28d7710e-46de-4994-dae6-2fe2820aad58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "can you summarize the first half of latex tutorial?\n",
            "In the first half of the LaTeX tutorial, we covered the basics of creating LaTeX documents. This included information on document classes like article, report, and book, as well as adding a title, author, and date to the document. We also discussed creating content in the document using the \\texttt{maketitle} command and adding images. Additionally, we touched on bibliography management, compilation steps, and creating a simple LaTeX document. Lastly, we mentioned some popular standalone LaTeX editors and advanced topics like macros, Beamer for presentations, and pgfplots for creating graphs and figures.\n",
            "bue\n",
            "I'm not sure what you mean by \"bue.\" If you have a question or need assistance with something related to LaTeX, feel free to ask!\n",
            "bye\n"
          ]
        }
      ],
      "source": [
        "#This is the chatting part of the application. WE are using a simple input terminal based chat for now.\n",
        "# Get the user's input from the text input field\n",
        "user_input = input()\n",
        "\n",
        "# If there is user input, search for a response using the search_db function; the user can terminate the chat by entering 'Bye'\n",
        "while user_input not in ['Bye', 'bye', 'Bye!']:\n",
        "  output = search_db(db, user_input)\n",
        "  print(output)\n",
        "  user_input = input()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully created and tested a LLM powered custom chatbot that is able to answer queries relevant to the given github repo. However, we have been chatting via the Python terminal itself. Next, we will see how to use Streamlit to create a nice GUI based front end for this application in a different notebook."
      ],
      "metadata": {
        "id": "I4h-6YKMJuh-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdJNd3L53thX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgeEebICJz1xZH9q+KfJVG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}